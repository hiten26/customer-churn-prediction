{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler , LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Additional imports\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score , confusion_matrix , classification_report\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "    \n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/raw/customer_data/ecomm-data.csv\")\n",
    "data.drop('CustomerID' , axis = 1 , inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Churn                          0.00\n",
       "Tenure                         4.69\n",
       "PreferredLoginDevice           0.00\n",
       "CityTier                       0.00\n",
       "WarehouseToHome                4.46\n",
       "PreferredPaymentMode           0.00\n",
       "Gender                         0.00\n",
       "HourSpendOnApp                 4.53\n",
       "NumberOfDeviceRegistered       0.00\n",
       "PreferedOrderCat               0.00\n",
       "SatisfactionScore              0.00\n",
       "MaritalStatus                  0.00\n",
       "NumberOfAddress                0.00\n",
       "Complain                       0.00\n",
       "OrderAmountHikeFromlastYear    4.71\n",
       "CouponUsed                     4.55\n",
       "OrderCount                     4.58\n",
       "DaySinceLastOrder              5.45\n",
       "CashbackAmount                 0.00\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round((data.isnull().sum()*100 / data.shape[0]),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fit and save encoders and imputer during training\n",
    "def fit_and_save_preprocessors(train_data, categorical_columns, impute_strategy, scaler_type):\n",
    "    encoder_dict = {}\n",
    "\n",
    "    for col in categorical_columns:\n",
    "        encoder = LabelEncoder()\n",
    "        encoder.fit(train_data[col].astype(str))  # Ensure categorical is a string\n",
    "        encoder_dict[col] = encoder\n",
    "        train_data[col] = encoder.transform(train_data[col])\n",
    "\n",
    "    imputer = SimpleImputer(strategy=impute_strategy)\n",
    "    if scaler_type == 'MinMax':\n",
    "        scaler = MinMaxScaler()\n",
    "    elif scaler_type == 'Standard':\n",
    "        scaler = StandardScaler()\n",
    "    elif scaler_type == 'Robust':\n",
    "        scaler = RobustScaler()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid scaler_type. Choose from 'MinMax', 'Standard', or 'Robust'.\")\n",
    "\n",
    "    \n",
    "    for col in train_data.columns:\n",
    "        if col not in categorical_columns:\n",
    "            train_data[col] = imputer.fit_transform(train_data[[col]])\n",
    "\n",
    "    joblib.dump(encoder_dict, 'encoders.pkl')\n",
    "    joblib.dump(imputer, 'imputer.pkl')\n",
    "\n",
    "    # Apply scaler to the entire DataFrame and convert it back to a DataFrame\n",
    "    train_data_scaled = pd.DataFrame(scaler.fit_transform(train_data), columns=train_data.columns)\n",
    "\n",
    "    joblib.dump(scaler, 'scaler.pkl')\n",
    "\n",
    "\n",
    "    return train_data_scaled\n",
    "\n",
    "# Function to apply encoders and imputer during testing\n",
    "def apply_preprocessors(test_data):\n",
    "    loaded_encoders = joblib.load('encoders.pkl')\n",
    "    loaded_imputer = joblib.load('imputer.pkl')\n",
    "    loaded_scaler = joblib.load('scaler.pkl')\n",
    "\n",
    "    for col in test_data.columns:\n",
    "        if col in loaded_encoders:\n",
    "            encoder = loaded_encoders[col]\n",
    "            test_data[col] = encoder.transform(test_data[col])\n",
    "        elif col in loaded_imputer:\n",
    "            test_data[col] = loaded_imputer[col].transform(test_data[[col]])\n",
    "        test_data[col] = loaded_scaler.transform(test_data[col])\n",
    "    \n",
    "    return test_data\n",
    "\n",
    "# Detect outliers and remove for training data \n",
    "def handle_outliers(df , column_name):\n",
    "  Q1 = df[column_name].quantile(0.25)\n",
    "  Q3 = df[column_name].quantile(0.75)\n",
    "  IQR = Q3 - Q1\n",
    "\n",
    "  # Define Upper and lower boundaries\n",
    "  Upper = Q3 + IQR * 1.5\n",
    "  lower = Q1 - IQR * 1.5\n",
    "\n",
    "  # lets make filter for col values\n",
    "  new_df = df[ (df[column_name] > lower) & (df[column_name] < Upper) ]\n",
    "\n",
    "  return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Churn                          0.00\n",
       "Tenure                         4.69\n",
       "PreferredLoginDevice           0.00\n",
       "CityTier                       0.00\n",
       "WarehouseToHome                4.46\n",
       "PreferredPaymentMode           0.00\n",
       "Gender                         0.00\n",
       "HourSpendOnApp                 4.53\n",
       "NumberOfDeviceRegistered       0.00\n",
       "PreferedOrderCat               0.00\n",
       "SatisfactionScore              0.00\n",
       "MaritalStatus                  0.00\n",
       "NumberOfAddress                0.00\n",
       "Complain                       0.00\n",
       "OrderAmountHikeFromlastYear    4.71\n",
       "CouponUsed                     4.55\n",
       "OrderCount                     4.58\n",
       "DaySinceLastOrder              5.45\n",
       "CashbackAmount                 0.00\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round((data.isnull().sum()*100 / data.shape[0]),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = data.select_dtypes(include=['object']).columns.tolist()\n",
    "data_processed = fit_and_save_preprocessors(data, categorical_columns=categorical_cols, impute_strategy=\"median\", scaler_type=\"MinMax\")\n",
    "\n",
    "# lets Give our Functions columns contains outlier\n",
    "cols_outliers = ['Tenure' , 'WarehouseToHome' , 'NumberOfAddress' , 'DaySinceLastOrder' , 'HourSpendOnApp' , 'NumberOfDeviceRegistered']\n",
    "for col in cols_outliers:\n",
    "    data_without_outliers = handle_outliers(data_processed , col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_data_balancing_technique(df):\n",
    "    X = df.drop('Churn' , axis = 1)\n",
    "    Y = df['Churn']\n",
    "\n",
    "    smt = SMOTETomek(random_state=42)\n",
    "    x_over , y_over = smt.fit_resample(X , Y)\n",
    "    return x_over, y_over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_balanced, y_balanced = apply_data_balancing_technique(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train , x_test , y_train , y_test = train_test_split(X_balanced , y_balanced , test_size = 0.30 , random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "logisreg_clf = LogisticRegression()\n",
    "svm_clf = SVC()\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "rf_clf = RandomForestClassifier()\n",
    "XGB_clf = XGBClassifier()\n",
    "ada_clf = AdaBoostClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m clf_name_list \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mLogistic Regression\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mSupport Vector Machine\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mDecision Tree\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mRandom Forest\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mXGBClassifier\u001b[39m\u001b[39m'\u001b[39m , \u001b[39m'\u001b[39m\u001b[39mAdaBoostClassifier\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m clf \u001b[39min\u001b[39;00m clf_list:\n\u001b[0;32m----> 5\u001b[0m     clf\u001b[39m.\u001b[39;49mfit(x_train,y_train)\n",
      "File \u001b[0;32m~/personal-projects/github/customer-churn-prediction/.venv/lib/python3.11/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/personal-projects/github/customer-churn-prediction/.venv/lib/python3.11/site-packages/sklearn/svm/_base.py:250\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m[LibSVM]\u001b[39m\u001b[39m\"\u001b[39m, end\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    249\u001b[0m seed \u001b[39m=\u001b[39m rnd\u001b[39m.\u001b[39mrandint(np\u001b[39m.\u001b[39miinfo(\u001b[39m\"\u001b[39m\u001b[39mi\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mmax)\n\u001b[0;32m--> 250\u001b[0m fit(X, y, sample_weight, solver_type, kernel, random_seed\u001b[39m=\u001b[39;49mseed)\n\u001b[1;32m    251\u001b[0m \u001b[39m# see comment on the other call to np.iinfo in this file\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape_fit_ \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(X, \u001b[39m\"\u001b[39m\u001b[39mshape\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m (n_samples,)\n",
      "File \u001b[0;32m~/personal-projects/github/customer-churn-prediction/.venv/lib/python3.11/site-packages/sklearn/svm/_base.py:329\u001b[0m, in \u001b[0;36mBaseLibSVM._dense_fit\u001b[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[1;32m    315\u001b[0m libsvm\u001b[39m.\u001b[39mset_verbosity_wrap(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[1;32m    317\u001b[0m \u001b[39m# we don't pass **self.get_params() to allow subclasses to\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[39m# add other parameters to __init__\u001b[39;00m\n\u001b[1;32m    319\u001b[0m (\n\u001b[1;32m    320\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msupport_,\n\u001b[1;32m    321\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msupport_vectors_,\n\u001b[1;32m    322\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_support,\n\u001b[1;32m    323\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdual_coef_,\n\u001b[1;32m    324\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintercept_,\n\u001b[1;32m    325\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_probA,\n\u001b[1;32m    326\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_probB,\n\u001b[1;32m    327\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_status_,\n\u001b[1;32m    328\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_iter,\n\u001b[0;32m--> 329\u001b[0m ) \u001b[39m=\u001b[39m libsvm\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m    330\u001b[0m     X,\n\u001b[1;32m    331\u001b[0m     y,\n\u001b[1;32m    332\u001b[0m     svm_type\u001b[39m=\u001b[39;49msolver_type,\n\u001b[1;32m    333\u001b[0m     sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m    334\u001b[0m     \u001b[39m# TODO(1.4): Replace \"_class_weight\" with \"class_weight_\"\u001b[39;49;00m\n\u001b[1;32m    335\u001b[0m     class_weight\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m_class_weight\u001b[39;49m\u001b[39m\"\u001b[39;49m, np\u001b[39m.\u001b[39;49mempty(\u001b[39m0\u001b[39;49m)),\n\u001b[1;32m    336\u001b[0m     kernel\u001b[39m=\u001b[39;49mkernel,\n\u001b[1;32m    337\u001b[0m     C\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mC,\n\u001b[1;32m    338\u001b[0m     nu\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnu,\n\u001b[1;32m    339\u001b[0m     probability\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprobability,\n\u001b[1;32m    340\u001b[0m     degree\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdegree,\n\u001b[1;32m    341\u001b[0m     shrinking\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshrinking,\n\u001b[1;32m    342\u001b[0m     tol\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtol,\n\u001b[1;32m    343\u001b[0m     cache_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcache_size,\n\u001b[1;32m    344\u001b[0m     coef0\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcoef0,\n\u001b[1;32m    345\u001b[0m     gamma\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_gamma,\n\u001b[1;32m    346\u001b[0m     epsilon\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepsilon,\n\u001b[1;32m    347\u001b[0m     max_iter\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_iter,\n\u001b[1;32m    348\u001b[0m     random_seed\u001b[39m=\u001b[39;49mrandom_seed,\n\u001b[1;32m    349\u001b[0m )\n\u001b[1;32m    351\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_warn_from_fit_status()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "clf_list = [logisreg_clf, svm_clf, dt_clf, rf_clf, XGB_clf, ada_clf]\n",
    "clf_name_list = ['Logistic Regression', 'Support Vector Machine', 'Decision Tree', 'Random Forest', 'XGBClassifier' , 'AdaBoostClassifier']\n",
    "\n",
    "for clf in clf_list:\n",
    "    clf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: Logistic Regression\n",
      "Trainning Score: 0.7863899908452853\n",
      "Test Score: 0.7893238434163701\n",
      "f1-score Train: 0.7898528970279196\n",
      "f1-score Test: 0.7943015983321752\n",
      "                                                            \n",
      "************************************************************\n",
      "                                                            \n",
      "Using model: Support Vector Machine\n",
      "Trainning Score: 0.7703692401586817\n",
      "Test Score: 0.7640569395017793\n",
      "f1-score Train: 0.7841055802610816\n",
      "f1-score Test: 0.7795144662454273\n",
      "                                                            \n",
      "************************************************************\n",
      "                                                            \n",
      "Using model: Decision Tree\n",
      "Trainning Score: 1.0\n",
      "Test Score: 0.9615658362989323\n",
      "f1-score Train: 1.0\n",
      "f1-score Test: 0.9617834394904459\n",
      "                                                            \n",
      "************************************************************\n",
      "                                                            \n",
      "Using model: Random Forest\n",
      "Trainning Score: 1.0\n",
      "Test Score: 0.9829181494661922\n",
      "f1-score Train: 1.0\n",
      "f1-score Test: 0.9830985915492958\n",
      "                                                            \n",
      "************************************************************\n",
      "                                                            \n",
      "Using model: XGBClassifier\n",
      "Trainning Score: 1.0\n",
      "Test Score: 0.9829181494661922\n",
      "f1-score Train: 1.0\n",
      "f1-score Test: 0.983026874115983\n",
      "                                                            \n",
      "************************************************************\n",
      "                                                            \n",
      "Using model: AdaBoostClassifier\n",
      "Trainning Score: 0.9015868172108636\n",
      "Test Score: 0.8939501779359431\n",
      "f1-score Train: 0.9012099862153469\n",
      "f1-score Test: 0.893799002138275\n",
      "                                                            \n",
      "************************************************************\n",
      "                                                            \n"
     ]
    }
   ],
   "source": [
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "for clf,name in zip(clf_list,clf_name_list):\n",
    "    y_pred_train = clf.predict(x_train)\n",
    "    y_pred_test = clf.predict(x_test)\n",
    "    print(f'Using model: {name}')\n",
    "    print(f'Trainning Score: {clf.score(x_train, y_train)}')\n",
    "    print(f'Test Score: {clf.score(x_test, y_test)}')\n",
    "    print(f'f1-score Train: {f1_score(y_train, y_pred_train)}')\n",
    "    print(f'f1-score Test: {f1_score(y_test, y_pred_test)}')\n",
    "    train_acc_list.append(accuracy_score(y_train, y_pred_train))\n",
    "    test_acc_list.append(accuracy_score(y_test, y_pred_test))\n",
    "    print(' ' * 60)\n",
    "    print('*' * 60)\n",
    "    print(' ' * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/10/06 09:37:57 INFO mlflow.tracking.fluent: Experiment with name 'nyc-taxi-experiment' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='/Users/hiten/personal-projects/github/customer-churn-prediction/customer_churn_prediction/notebooks/mlruns/1', creation_time=1696577877827, experiment_id='1', last_update_time=1696577877827, lifecycle_stage='active', name='nyc-taxi-experiment', tags={}>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "mlflow.set_experiment(\"nyc-taxi-experiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataframe(filename):\n",
    "    if filename.endswith('.csv'):\n",
    "        df = pd.read_csv(filename)\n",
    "        df.lpep_dropoff_datetime = pd.to_datetime(df.lpep_dropoff_datetime)\n",
    "        df.lpep_pickup_datetime = pd.to_datetime(df.lpep_pickup_datetime)\n",
    "    elif filename.endswith('.parquet'):\n",
    "        df = pd.read_parquet(filename)\n",
    "\n",
    "    df['duration'] = df.tpep_dropoff_datetime - df.tpep_pickup_datetime\n",
    "    df.duration = df.duration.apply(lambda td: td.total_seconds() / 60)\n",
    "\n",
    "    df = df[(df.duration >= 1) & (df.duration <= 60)]\n",
    "\n",
    "    categorical = ['PULocationID', 'DOLocationID']\n",
    "    df[categorical] = df[categorical].astype(str)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = read_dataframe('../../../../MLOpsEngineer/mlops-zoomcamp/data/yellow_tripdata_2023-05.parquet')\n",
    "df_val = read_dataframe('../../../../MLOpsEngineer/mlops-zoomcamp//data/yellow_tripdata_2023-06.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import Lasso\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['PU_DO'] = df_train['PULocationID'] + '_' + df_train['DOLocationID']\n",
    "df_val['PU_DO'] = df_val['PULocationID'] + '_' + df_val['DOLocationID']\n",
    "\n",
    "categorical = ['PU_DO'] #'PULocationID', 'DOLocationID']\n",
    "numerical = ['trip_distance']\n",
    "\n",
    "dv = DictVectorizer()\n",
    "\n",
    "train_dicts = df_train[categorical + numerical].to_dict(orient='records')\n",
    "X_train = dv.fit_transform(train_dicts)\n",
    "\n",
    "val_dicts = df_val[categorical + numerical].to_dict(orient='records')\n",
    "X_val = dv.transform(val_dicts)\n",
    "\n",
    "target = 'duration'\n",
    "y_train = df_train[target].values\n",
    "y_val = df_val[target].values\n",
    "\n",
    "with mlflow.start_run():\n",
    "\n",
    "    mlflow.set_tag(\"developer\", \"Hitendra\")\n",
    "\n",
    "    mlflow.log_param(\"train-data-path\", \"../data/yellow_tripdata_2023-05.parquet\")\n",
    "    mlflow.log_param(\"valid-data-path\", \"../data/yellow_tripdata_2023-06.parquet\")\n",
    "\n",
    "    alpha = 0.1\n",
    "    mlflow.log_param(\"alpha\", alpha)\n",
    "    lr = Lasso(alpha)\n",
    "    lr.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = lr.predict(X_val)\n",
    "    rmse = mean_squared_error(y_val, y_pred, squared=False)\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "\n",
    "    mlflow.log_artifact(local_path=\"../models/lin_reg.bin\", artifact_path=\"models_pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler , LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Additional imports\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score , confusion_matrix , classification_report\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "    \n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/10/06 10:13:15 INFO mlflow.tracking.fluent: Experiment with name 'customer_churn_model' does not exist. Creating a new experiment.\n",
      "INFO:root:Running ML pipeline for model: Logistic Regression, impute strategy: mean, scaler type: MinMax\n",
      "INFO:root:Starting preprocessing for impute strategy: mean, scaler type: MinMax\n",
      "INFO:root:Running ML pipeline for model: Logistic Regression, impute strategy: mean, scaler type: Standard\n",
      "INFO:root:Starting preprocessing for impute strategy: mean, scaler type: Standard\n",
      "INFO:root:Running ML pipeline for model: Logistic Regression, impute strategy: mean, scaler type: Robust\n",
      "INFO:root:Starting preprocessing for impute strategy: mean, scaler type: Robust\n",
      "INFO:root:Running ML pipeline for model: Logistic Regression, impute strategy: median, scaler type: MinMax\n",
      "INFO:root:Starting preprocessing for impute strategy: median, scaler type: MinMax\n",
      "INFO:root:Running ML pipeline for model: Logistic Regression, impute strategy: median, scaler type: Standard\n",
      "INFO:root:Starting preprocessing for impute strategy: median, scaler type: Standard\n",
      "INFO:root:Running ML pipeline for model: Logistic Regression, impute strategy: median, scaler type: Robust\n",
      "INFO:root:Starting preprocessing for impute strategy: median, scaler type: Robust\n",
      "INFO:root:Running ML pipeline for model: Support Vector Machine, impute strategy: mean, scaler type: MinMax\n",
      "INFO:root:Starting preprocessing for impute strategy: mean, scaler type: MinMax\n",
      "INFO:root:Running ML pipeline for model: Support Vector Machine, impute strategy: mean, scaler type: Standard\n",
      "INFO:root:Starting preprocessing for impute strategy: mean, scaler type: Standard\n",
      "INFO:root:Running ML pipeline for model: Support Vector Machine, impute strategy: mean, scaler type: Robust\n",
      "INFO:root:Starting preprocessing for impute strategy: mean, scaler type: Robust\n",
      "INFO:root:Running ML pipeline for model: Support Vector Machine, impute strategy: median, scaler type: MinMax\n",
      "INFO:root:Starting preprocessing for impute strategy: median, scaler type: MinMax\n",
      "INFO:root:Running ML pipeline for model: Support Vector Machine, impute strategy: median, scaler type: Standard\n",
      "INFO:root:Starting preprocessing for impute strategy: median, scaler type: Standard\n",
      "INFO:root:Running ML pipeline for model: Support Vector Machine, impute strategy: median, scaler type: Robust\n",
      "INFO:root:Starting preprocessing for impute strategy: median, scaler type: Robust\n",
      "INFO:root:Running ML pipeline for model: Decision Tree, impute strategy: mean, scaler type: MinMax\n",
      "INFO:root:Starting preprocessing for impute strategy: mean, scaler type: MinMax\n",
      "INFO:root:Running ML pipeline for model: Decision Tree, impute strategy: mean, scaler type: Standard\n",
      "INFO:root:Starting preprocessing for impute strategy: mean, scaler type: Standard\n",
      "INFO:root:Running ML pipeline for model: Decision Tree, impute strategy: mean, scaler type: Robust\n",
      "INFO:root:Starting preprocessing for impute strategy: mean, scaler type: Robust\n",
      "INFO:root:Running ML pipeline for model: Decision Tree, impute strategy: median, scaler type: MinMax\n",
      "INFO:root:Starting preprocessing for impute strategy: median, scaler type: MinMax\n",
      "INFO:root:Running ML pipeline for model: Decision Tree, impute strategy: median, scaler type: Standard\n",
      "INFO:root:Starting preprocessing for impute strategy: median, scaler type: Standard\n",
      "INFO:root:Running ML pipeline for model: Decision Tree, impute strategy: median, scaler type: Robust\n",
      "INFO:root:Starting preprocessing for impute strategy: median, scaler type: Robust\n",
      "INFO:root:Running ML pipeline for model: Random Forest, impute strategy: mean, scaler type: MinMax\n",
      "INFO:root:Starting preprocessing for impute strategy: mean, scaler type: MinMax\n",
      "INFO:root:Running ML pipeline for model: Random Forest, impute strategy: mean, scaler type: Standard\n",
      "INFO:root:Starting preprocessing for impute strategy: mean, scaler type: Standard\n",
      "INFO:root:Running ML pipeline for model: Random Forest, impute strategy: mean, scaler type: Robust\n",
      "INFO:root:Starting preprocessing for impute strategy: mean, scaler type: Robust\n",
      "INFO:root:Running ML pipeline for model: Random Forest, impute strategy: median, scaler type: MinMax\n",
      "INFO:root:Starting preprocessing for impute strategy: median, scaler type: MinMax\n",
      "INFO:root:Running ML pipeline for model: Random Forest, impute strategy: median, scaler type: Standard\n",
      "INFO:root:Starting preprocessing for impute strategy: median, scaler type: Standard\n",
      "INFO:root:Running ML pipeline for model: Random Forest, impute strategy: median, scaler type: Robust\n",
      "INFO:root:Starting preprocessing for impute strategy: median, scaler type: Robust\n",
      "INFO:root:Running ML pipeline for model: XGBClassifier, impute strategy: mean, scaler type: MinMax\n",
      "INFO:root:Starting preprocessing for impute strategy: mean, scaler type: MinMax\n",
      "INFO:root:Running ML pipeline for model: XGBClassifier, impute strategy: mean, scaler type: Standard\n",
      "INFO:root:Starting preprocessing for impute strategy: mean, scaler type: Standard\n",
      "INFO:root:Running ML pipeline for model: XGBClassifier, impute strategy: mean, scaler type: Robust\n",
      "INFO:root:Starting preprocessing for impute strategy: mean, scaler type: Robust\n",
      "INFO:root:Running ML pipeline for model: XGBClassifier, impute strategy: median, scaler type: MinMax\n",
      "INFO:root:Starting preprocessing for impute strategy: median, scaler type: MinMax\n",
      "INFO:root:Running ML pipeline for model: XGBClassifier, impute strategy: median, scaler type: Standard\n",
      "INFO:root:Starting preprocessing for impute strategy: median, scaler type: Standard\n",
      "INFO:root:Running ML pipeline for model: XGBClassifier, impute strategy: median, scaler type: Robust\n",
      "INFO:root:Starting preprocessing for impute strategy: median, scaler type: Robust\n",
      "INFO:root:Running ML pipeline for model: AdaBoostClassifier, impute strategy: mean, scaler type: MinMax\n",
      "INFO:root:Starting preprocessing for impute strategy: mean, scaler type: MinMax\n",
      "INFO:root:Running ML pipeline for model: AdaBoostClassifier, impute strategy: mean, scaler type: Standard\n",
      "INFO:root:Starting preprocessing for impute strategy: mean, scaler type: Standard\n",
      "INFO:root:Running ML pipeline for model: AdaBoostClassifier, impute strategy: mean, scaler type: Robust\n",
      "INFO:root:Starting preprocessing for impute strategy: mean, scaler type: Robust\n",
      "INFO:root:Running ML pipeline for model: AdaBoostClassifier, impute strategy: median, scaler type: MinMax\n",
      "INFO:root:Starting preprocessing for impute strategy: median, scaler type: MinMax\n",
      "INFO:root:Running ML pipeline for model: AdaBoostClassifier, impute strategy: median, scaler type: Standard\n",
      "INFO:root:Starting preprocessing for impute strategy: median, scaler type: Standard\n",
      "INFO:root:Running ML pipeline for model: AdaBoostClassifier, impute strategy: median, scaler type: Robust\n",
      "INFO:root:Starting preprocessing for impute strategy: median, scaler type: Robust\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from imblearn.combine import SMOTETomek\n",
    "import joblib\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import logging\n",
    "\n",
    "# Set up logging configuration\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "# Set MLflow experiment name\n",
    "mlflow.set_experiment(\"customer_churn_model\")\n",
    "\n",
    "\n",
    "\n",
    "# Function to fit and save preprocessors during training\n",
    "def fit_and_save_preprocessors(train_data, impute_strategy, scaler_type):\n",
    "    logging.info(f\"Starting preprocessing for impute strategy: {impute_strategy}, scaler type: {scaler_type}\")\n",
    "\n",
    "    # Categorical columns in the dataset (if any)\n",
    "    categorical_columns = data.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "    encoder_dict = {}\n",
    "    imputer = SimpleImputer(strategy=impute_strategy)\n",
    "\n",
    "    for col in categorical_columns:\n",
    "        encoder = LabelEncoder()\n",
    "        encoder.fit(train_data[col].astype(str))  # Ensure categorical is a string\n",
    "        encoder_dict[col] = encoder\n",
    "        train_data[col] = encoder.fit_transform(train_data[col])\n",
    "\n",
    "    for col in train_data.columns:\n",
    "        if col not in categorical_columns:\n",
    "            train_data[col] = imputer.fit_transform(train_data[[col]])\n",
    "\n",
    "    if scaler_type == 'MinMax':\n",
    "        scaler = MinMaxScaler()\n",
    "    elif scaler_type == 'Standard':\n",
    "        scaler = StandardScaler()\n",
    "    elif scaler_type == 'Robust':\n",
    "        scaler = RobustScaler()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid scaler_type. Choose from 'MinMax', 'Standard', or 'Robust'.\")\n",
    "\n",
    "    # Separate the target variable \"Churn\" from the features\n",
    "    X_train = train_data.drop(columns=['Churn'])\n",
    "    y_train = train_data['Churn']\n",
    "\n",
    "    # Apply the scaler to the feature columns only\n",
    "    X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "\n",
    "    # Combine the scaled features with the target variable\n",
    "    train_data_scaled = pd.concat([X_train_scaled, y_train], axis=1)\n",
    "\n",
    "    joblib.dump(encoder_dict, 'encoders.pkl')\n",
    "    joblib.dump(imputer, 'imputer.pkl')\n",
    "    joblib.dump(scaler, 'scaler.pkl')\n",
    "\n",
    "    return train_data_scaled\n",
    "\n",
    "# Function to apply encoders and imputer during testing\n",
    "def apply_preprocessors(test_data):\n",
    "    loaded_encoders = joblib.load('encoders.pkl')\n",
    "    loaded_imputer = joblib.load('imputer.pkl')\n",
    "    loaded_scaler = joblib.load('scaler.pkl')\n",
    "\n",
    "    for col in test_data.columns:\n",
    "        if col in loaded_encoders:\n",
    "            encoder = loaded_encoders[col]\n",
    "            test_data[col] = encoder.transform(test_data[col])\n",
    "        elif col in loaded_imputer:\n",
    "            test_data[col] = loaded_imputer[col].transform(test_data[[col]])\n",
    "\n",
    "    test_data_scaled = pd.DataFrame(loaded_scaler.transform(test_data), columns=test_data.columns)\n",
    "\n",
    "    return test_data_scaled\n",
    "\n",
    "# Function to handle outliers for training data\n",
    "def handle_outliers(df, column_name):\n",
    "    Q1 = df[column_name].quantile(0.25)\n",
    "    Q3 = df[column_name].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    Upper = Q3 + IQR * 1.5\n",
    "    lower = Q1 - IQR * 1.5\n",
    "    new_df = df[(df[column_name] > lower) & (df[column_name] < Upper)]\n",
    "    return new_df\n",
    "\n",
    "# Function to apply data balancing technique\n",
    "def apply_data_balancing_technique(df):\n",
    "    X = df.drop('Churn', axis=1)\n",
    "    Y = df['Churn']\n",
    "    smt = SMOTETomek(random_state=42)\n",
    "    x_over, y_over = smt.fit_resample(X, Y)\n",
    "    return x_over, y_over\n",
    "\n",
    "# Create a list of classifiers\n",
    "def get_classifiers():\n",
    "    logisreg_clf = LogisticRegression()\n",
    "    svm_clf = SVC()\n",
    "    dt_clf = DecisionTreeClassifier()\n",
    "    rf_clf = RandomForestClassifier()\n",
    "    XGB_clf = XGBClassifier()\n",
    "    ada_clf = AdaBoostClassifier()\n",
    "    return [logisreg_clf, svm_clf, dt_clf, rf_clf, XGB_clf, ada_clf]\n",
    "\n",
    "# Main function to run the ML pipeline\n",
    "def run_ml_pipeline(data, classifier, classifier_name, impute_strategy, scaler_type):\n",
    "\n",
    "    logging.info(f\"Running ML pipeline for model: {classifier_name}, impute strategy: {impute_strategy}, scaler type: {scaler_type}\")\n",
    "\n",
    "    # Fit and save preprocessors\n",
    "    data_processed = fit_and_save_preprocessors(data, impute_strategy, scaler_type)\n",
    "\n",
    "    # Handle outliers\n",
    "    cols_outliers = ['Tenure', 'WarehouseToHome', 'NumberOfAddress', 'DaySinceLastOrder', 'HourSpendOnApp', 'NumberOfDeviceRegistered']\n",
    "    for col in cols_outliers:\n",
    "        data_processed = handle_outliers(data_processed, col)\n",
    "\n",
    "    # Apply data balancing technique\n",
    "    X_balanced, y_balanced = apply_data_balancing_technique(data_processed)\n",
    "\n",
    "    # Split the data into train and test sets\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X_balanced, y_balanced, test_size=0.30, random_state=42)\n",
    "\n",
    "    with mlflow.start_run() as run:\n",
    "        classifier.fit(x_train, y_train)\n",
    "        y_pred_train = classifier.predict(x_train)\n",
    "        y_pred_test = classifier.predict(x_test)\n",
    "\n",
    "        mlflow.log_params({'Impute Strategy': impute_strategy, 'Scaler Type': scaler_type, \"Model\":classifier_name})\n",
    "        mlflow.log_metric('Training Accuracy', accuracy_score(y_train, y_pred_train))\n",
    "        mlflow.log_metric('Test Accuracy', accuracy_score(y_test, y_pred_test))\n",
    "        mlflow.log_metric('Training F1-Score', f1_score(y_train, y_pred_train))\n",
    "        mlflow.log_metric('Test F1-Score', f1_score(y_test, y_pred_test))\n",
    "\n",
    "        mlflow.sklearn.log_model(clf, name)\n",
    "\n",
    "# Sample data\n",
    "data = pd.read_csv(\"../data/raw/customer_data/ecomm-data.csv\")\n",
    "del data['CustomerID']\n",
    "\n",
    "# List of impute strategies and scaler types to try\n",
    "impute_strategies = [\"mean\", \"median\"]\n",
    "scaler_types = [\"MinMax\", \"Standard\", \"Robust\"]\n",
    "\n",
    "# Create a list of classifiers\n",
    "classifiers = get_classifiers()\n",
    "clf_name_list = ['Logistic Regression', 'Support Vector Machine', 'Decision Tree', 'Random Forest', 'XGBClassifier', 'AdaBoostClassifier']\n",
    "\n",
    "# Run the ML pipeline for different combinations of impute strategies and scaler types\n",
    "for clf, name in zip(classifiers, clf_name_list):\n",
    "    for impute_strategy in impute_strategies:\n",
    "        for scaler_type in scaler_types:\n",
    "            run_ml_pipeline(data.copy(), classifier=clf, classifier_name=name, impute_strategy=impute_strategy, scaler_type=scaler_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/10/06 10:59:16 INFO mlflow.tracking.fluent: Experiment with name 'customer_churn_xgboost_model_hyperparameter_tuning' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='/Users/hiten/personal-projects/github/customer-churn-prediction/customer_churn_prediction/notebooks/mlruns/5', creation_time=1696582756621, experiment_id='5', last_update_time=1696582756621, lifecycle_stage='active', name='customer_churn_xgboost_model_hyperparameter_tuning', tags={}>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "# Set MLflow experiment name\n",
    "mlflow.set_experiment(\"customer_churn_xgboost_model_hyperparameter_tuning\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Starting preprocessing for impute strategy: median, scaler type: Robust\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.001536 seconds\n",
      "INFO:hyperopt.tpe:TPE using 0 trials\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [00:00<00:40,  1.20trial/s, best loss: -0.9812476081132798]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.002361 seconds\n",
      "INFO:hyperopt.tpe:TPE using 1/1 trials with best loss -0.981248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/50 [00:01<00:29,  1.65trial/s, best loss: -0.9812476081132798]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.002103 seconds\n",
      "INFO:hyperopt.tpe:TPE using 2/2 trials with best loss -0.981248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3/50 [00:02<00:50,  1.07s/trial, best loss: -0.98468606431853]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.002869 seconds\n",
      "INFO:hyperopt.tpe:TPE using 3/3 trials with best loss -0.984686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4/50 [00:03<00:39,  1.17trial/s, best loss: -0.988108937476026]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.002390 seconds\n",
      "INFO:hyperopt.tpe:TPE using 4/4 trials with best loss -0.988109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5/50 [00:04<00:49,  1.09s/trial, best loss: -0.988108937476026]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.002140 seconds\n",
      "INFO:hyperopt.tpe:TPE using 5/5 trials with best loss -0.988109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6/50 [00:06<00:52,  1.18s/trial, best loss: -0.988108937476026]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.002332 seconds\n",
      "INFO:hyperopt.tpe:TPE using 6/6 trials with best loss -0.988109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 7/50 [00:08<01:01,  1.43s/trial, best loss: -0.988108937476026]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.002697 seconds\n",
      "INFO:hyperopt.tpe:TPE using 7/7 trials with best loss -0.988109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 8/50 [00:09<00:55,  1.32s/trial, best loss: -0.988108937476026]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.003359 seconds\n",
      "INFO:hyperopt.tpe:TPE using 8/8 trials with best loss -0.988109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 9/50 [00:12<01:13,  1.79s/trial, best loss: -0.988108937476026]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.002847 seconds\n",
      "INFO:hyperopt.tpe:TPE using 9/9 trials with best loss -0.988109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20%|██        | 10/50 [00:15<01:32,  2.31s/trial, best loss: -0.988108937476026]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.002646 seconds\n",
      "INFO:hyperopt.tpe:TPE using 10/10 trials with best loss -0.988109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [00:16<01:13,  1.90s/trial, best loss: -0.988108937476026]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.002589 seconds\n",
      "INFO:hyperopt.tpe:TPE using 11/11 trials with best loss -0.988109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 12/50 [00:16<00:51,  1.36s/trial, best loss: -0.988108937476026]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.002413 seconds\n",
      "INFO:hyperopt.tpe:TPE using 12/12 trials with best loss -0.988109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 13/50 [00:19<01:04,  1.74s/trial, best loss: -0.988108937476026]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.002907 seconds\n",
      "INFO:hyperopt.tpe:TPE using 13/13 trials with best loss -0.988109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 14/50 [00:21<01:02,  1.74s/trial, best loss: -0.988108937476026]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.002435 seconds\n",
      "INFO:hyperopt.tpe:TPE using 14/14 trials with best loss -0.988109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 30%|███       | 15/50 [00:23<01:09,  1.99s/trial, best loss: -0.988108937476026]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.003894 seconds\n",
      "INFO:hyperopt.tpe:TPE using 15/15 trials with best loss -0.988109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 16/50 [00:25<01:11,  2.09s/trial, best loss: -0.988108937476026]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.002689 seconds\n",
      "INFO:hyperopt.tpe:TPE using 16/16 trials with best loss -0.988109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 17/50 [00:28<01:08,  2.09s/trial, best loss: -0.988108937476026]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.002616 seconds\n",
      "INFO:hyperopt.tpe:TPE using 17/17 trials with best loss -0.988109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 18/50 [00:29<00:58,  1.83s/trial, best loss: -0.988108937476026]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.004062 seconds\n",
      "INFO:hyperopt.tpe:TPE using 18/18 trials with best loss -0.988109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 19/50 [00:30<00:49,  1.60s/trial, best loss: -0.988108937476026]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.002491 seconds\n",
      "INFO:hyperopt.tpe:TPE using 19/19 trials with best loss -0.988109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 40%|████      | 20/50 [00:31<00:41,  1.39s/trial, best loss: -0.988108937476026]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.002754 seconds\n",
      "INFO:hyperopt.tpe:TPE using 20/20 trials with best loss -0.988109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 21/50 [00:31<00:29,  1.03s/trial, best loss: -0.988108937476026]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.002512 seconds\n",
      "INFO:hyperopt.tpe:TPE using 21/21 trials with best loss -0.988109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 22/50 [00:33<00:34,  1.23s/trial, best loss: -0.988108937476026]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.002744 seconds\n",
      "INFO:hyperopt.tpe:TPE using 22/22 trials with best loss -0.988109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 23/50 [00:34<00:34,  1.28s/trial, best loss: -0.988108937476026]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.002598 seconds\n",
      "INFO:hyperopt.tpe:TPE using 23/23 trials with best loss -0.988109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 24/50 [00:35<00:30,  1.18s/trial, best loss: -0.988108937476026]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.002644 seconds\n",
      "INFO:hyperopt.tpe:TPE using 24/24 trials with best loss -0.988109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 25/50 [00:35<00:22,  1.11trial/s, best loss: -0.988108937476026]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.002637 seconds\n",
      "INFO:hyperopt.tpe:TPE using 25/25 trials with best loss -0.988109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 26/50 [00:37<00:26,  1.11s/trial, best loss: -0.988108937476026]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.002749 seconds\n",
      "INFO:hyperopt.tpe:TPE using 26/26 trials with best loss -0.988109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 27/50 [00:38<00:27,  1.20s/trial, best loss: -0.988108937476026]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.002637 seconds\n",
      "INFO:hyperopt.tpe:TPE using 27/27 trials with best loss -0.988109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 28/50 [00:39<00:22,  1.04s/trial, best loss: -0.988108937476026]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.002457 seconds\n",
      "INFO:hyperopt.tpe:TPE using 28/28 trials with best loss -0.988109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 29/50 [00:40<00:19,  1.05trial/s, best loss: -0.988108937476026]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.002477 seconds\n",
      "INFO:hyperopt.tpe:TPE using 29/29 trials with best loss -0.988109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 30/50 [00:40<00:15,  1.28trial/s, best loss: -0.988108937476026]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.002466 seconds\n",
      "INFO:hyperopt.tpe:TPE using 30/30 trials with best loss -0.988109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 31/50 [00:41<00:18,  1.03trial/s, best loss: -0.988108937476026]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.002929 seconds\n",
      "INFO:hyperopt.tpe:TPE using 31/31 trials with best loss -0.988109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 32/50 [00:42<00:14,  1.27trial/s, best loss: -0.988108937476026]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.002425 seconds\n",
      "INFO:hyperopt.tpe:TPE using 32/32 trials with best loss -0.988109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 33/50 [00:43<00:13,  1.30trial/s, best loss: -0.988108937476026]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.002518 seconds\n",
      "INFO:hyperopt.tpe:TPE using 33/33 trials with best loss -0.988109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 34/50 [00:44<00:16,  1.04s/trial, best loss: -0.988108937476026]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.002729 seconds\n",
      "INFO:hyperopt.tpe:TPE using 34/34 trials with best loss -0.988109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 35/50 [00:46<00:18,  1.26s/trial, best loss: -0.988108937476026]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.002597 seconds\n",
      "INFO:hyperopt.tpe:TPE using 35/35 trials with best loss -0.988109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 36/50 [00:48<00:19,  1.40s/trial, best loss: -0.988108937476026]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.002521 seconds\n",
      "INFO:hyperopt.tpe:TPE using 36/36 trials with best loss -0.988109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 37/50 [00:49<00:18,  1.39s/trial, best loss: -0.988108937476026]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.002417 seconds\n",
      "INFO:hyperopt.tpe:TPE using 37/37 trials with best loss -0.988109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 38/50 [00:50<00:16,  1.36s/trial, best loss: -0.988108937476026]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.002525 seconds\n",
      "INFO:hyperopt.tpe:TPE using 38/38 trials with best loss -0.988109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 39/50 [00:52<00:15,  1.44s/trial, best loss: -0.988108937476026]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.002599 seconds\n",
      "INFO:hyperopt.tpe:TPE using 39/39 trials with best loss -0.988109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 40/50 [00:53<00:12,  1.26s/trial, best loss: -0.988108937476026]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.002425 seconds\n",
      "INFO:hyperopt.tpe:TPE using 40/40 trials with best loss -0.988109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 41/50 [00:53<00:08,  1.08trial/s, best loss: -0.988108937476026]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.002530 seconds\n",
      "INFO:hyperopt.tpe:TPE using 41/41 trials with best loss -0.988109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 42/50 [00:55<00:10,  1.35s/trial, best loss: -0.988108937476026]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.002501 seconds\n",
      "INFO:hyperopt.tpe:TPE using 42/42 trials with best loss -0.988109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 43/50 [00:56<00:09,  1.30s/trial, best loss: -0.988108937476026]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.003713 seconds\n",
      "INFO:hyperopt.tpe:TPE using 43/43 trials with best loss -0.988109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 44/50 [00:58<00:07,  1.32s/trial, best loss: -0.988108937476026]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.002945 seconds\n",
      "INFO:hyperopt.tpe:TPE using 44/44 trials with best loss -0.988109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 45/50 [01:00<00:07,  1.56s/trial, best loss: -0.988108937476026]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.002908 seconds\n",
      "INFO:hyperopt.tpe:TPE using 45/45 trials with best loss -0.988109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 46/50 [01:01<00:05,  1.47s/trial, best loss: -0.988108937476026]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.002554 seconds\n",
      "INFO:hyperopt.tpe:TPE using 46/46 trials with best loss -0.988109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 47/50 [01:02<00:03,  1.24s/trial, best loss: -0.988108937476026]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.002684 seconds\n",
      "INFO:hyperopt.tpe:TPE using 47/47 trials with best loss -0.988109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 48/50 [01:02<00:02,  1.03s/trial, best loss: -0.988108937476026]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.002600 seconds\n",
      "INFO:hyperopt.tpe:TPE using 48/48 trials with best loss -0.988109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 49/50 [01:06<00:01,  1.70s/trial, best loss: -0.988108937476026]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.002596 seconds\n",
      "INFO:hyperopt.tpe:TPE using 49/49 trials with best loss -0.988109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:08<00:00,  1.37s/trial, best loss: -0.988108937476026]\n",
      "Best Hyperparameters: {'colsample_bytree': 0.7906157383171843, 'learning_rate': 0.15465495515036473, 'max_depth': 10.0, 'min_child_weight': 9.0, 'n_estimators': 141.0, 'subsample': 0.5149313721037773}\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import hp, tpe, fmin\n",
    "from hyperopt.pyll import scope\n",
    "\n",
    "# Define the search space for hyperparameters\n",
    "space = {\n",
    "    'n_estimators': scope.int(hp.quniform('n_estimators', 50, 200, 1)),\n",
    "    'max_depth': scope.int(hp.quniform('max_depth', 4, 100, 1)),\n",
    "    'learning_rate': hp.loguniform('learning_rate', -4, 0),\n",
    "    'min_child_weight': scope.int(hp.quniform('min_child_weight', 1, 10, 1)),\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1),\n",
    "    'objective': 'reg:linear',\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# Objective function to optimize\n",
    "def objective(params):\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_params(params)\n",
    "        model = XGBClassifier(**params)\n",
    "        model.fit(x_train, y_train)\n",
    "        y_pred = model.predict(x_test)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        mlflow.log_metric('f1', f1)\n",
    "        return -f1  # Hyperopt minimizes the objective function, so we use negative f1-score\n",
    "\n",
    "# Fit and save preprocessors\n",
    "data_processed = fit_and_save_preprocessors(data, impute_strategy, scaler_type)\n",
    "\n",
    "# Handle outliers\n",
    "cols_outliers = ['Tenure', 'WarehouseToHome', 'NumberOfAddress', 'DaySinceLastOrder', 'HourSpendOnApp', 'NumberOfDeviceRegistered']\n",
    "for col in cols_outliers:\n",
    "    data_processed = handle_outliers(data_processed, col)\n",
    "\n",
    "# Apply data balancing technique\n",
    "X_balanced, y_balanced = apply_data_balancing_technique(data_processed)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_balanced, y_balanced, test_size=0.30, random_state=42)\n",
    "\n",
    "# Use Hyperopt to search for the best hyperparameters\n",
    "best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=50)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Handle outliers\n",
    "cols_outliers = ['Tenure', 'WarehouseToHome', 'NumberOfAddress', 'DaySinceLastOrder', 'HourSpendOnApp', 'NumberOfDeviceRegistered']\n",
    "for col in cols_outliers:\n",
    "    data_processed = handle_outliers(data_processed, col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Churn</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>PreferredLoginDevice</th>\n",
       "      <th>CityTier</th>\n",
       "      <th>WarehouseToHome</th>\n",
       "      <th>PreferredPaymentMode</th>\n",
       "      <th>Gender</th>\n",
       "      <th>HourSpendOnApp</th>\n",
       "      <th>NumberOfDeviceRegistered</th>\n",
       "      <th>PreferedOrderCat</th>\n",
       "      <th>SatisfactionScore</th>\n",
       "      <th>MaritalStatus</th>\n",
       "      <th>NumberOfAddress</th>\n",
       "      <th>Complain</th>\n",
       "      <th>OrderAmountHikeFromlastYear</th>\n",
       "      <th>CouponUsed</th>\n",
       "      <th>OrderCount</th>\n",
       "      <th>DaySinceLastOrder</th>\n",
       "      <th>CashbackAmount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.731743</td>\n",
       "      <td>2.222345</td>\n",
       "      <td>-0.741002</td>\n",
       "      <td>0.100852</td>\n",
       "      <td>1.469771</td>\n",
       "      <td>-1.156091</td>\n",
       "      <td>0.325191</td>\n",
       "      <td>-1.227468</td>\n",
       "      <td>0.097069</td>\n",
       "      <td>-0.672900</td>\n",
       "      <td>-0.261904</td>\n",
       "      <td>-0.772992</td>\n",
       "      <td>1.251898</td>\n",
       "      <td>1.852616</td>\n",
       "      <td>1.584290</td>\n",
       "      <td>-1.312273</td>\n",
       "      <td>-0.405767</td>\n",
       "      <td>-0.699345</td>\n",
       "      <td>1.284833e-01</td>\n",
       "      <td>-0.350105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.731128</td>\n",
       "      <td>2.222345</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.509782</td>\n",
       "      <td>-0.715286</td>\n",
       "      <td>-0.916235</td>\n",
       "      <td>1.764521</td>\n",
       "      <td>0.814685</td>\n",
       "      <td>0.097069</td>\n",
       "      <td>0.303750</td>\n",
       "      <td>0.446658</td>\n",
       "      <td>-0.048392</td>\n",
       "      <td>1.251898</td>\n",
       "      <td>1.078430</td>\n",
       "      <td>1.584290</td>\n",
       "      <td>-0.197324</td>\n",
       "      <td>-0.946053</td>\n",
       "      <td>-0.699345</td>\n",
       "      <td>-1.278752e+00</td>\n",
       "      <td>-1.142957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.730513</td>\n",
       "      <td>2.222345</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.509782</td>\n",
       "      <td>-0.715286</td>\n",
       "      <td>1.722175</td>\n",
       "      <td>0.325191</td>\n",
       "      <td>0.814685</td>\n",
       "      <td>-1.320723</td>\n",
       "      <td>0.303750</td>\n",
       "      <td>0.446658</td>\n",
       "      <td>-0.048392</td>\n",
       "      <td>1.251898</td>\n",
       "      <td>0.691336</td>\n",
       "      <td>1.584290</td>\n",
       "      <td>-0.476062</td>\n",
       "      <td>-0.946053</td>\n",
       "      <td>-0.699345</td>\n",
       "      <td>-4.344109e-01</td>\n",
       "      <td>-1.163287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.729897</td>\n",
       "      <td>2.222345</td>\n",
       "      <td>-1.219847</td>\n",
       "      <td>1.509782</td>\n",
       "      <td>1.469771</td>\n",
       "      <td>-0.076741</td>\n",
       "      <td>0.325191</td>\n",
       "      <td>0.814685</td>\n",
       "      <td>-1.320723</td>\n",
       "      <td>0.303750</td>\n",
       "      <td>-0.261904</td>\n",
       "      <td>1.400807</td>\n",
       "      <td>1.251898</td>\n",
       "      <td>1.465523</td>\n",
       "      <td>-0.631198</td>\n",
       "      <td>2.032574</td>\n",
       "      <td>-0.946053</td>\n",
       "      <td>-0.699345</td>\n",
       "      <td>-4.344109e-01</td>\n",
       "      <td>-0.878673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.729282</td>\n",
       "      <td>2.222345</td>\n",
       "      <td>-1.219847</td>\n",
       "      <td>1.509782</td>\n",
       "      <td>-0.715286</td>\n",
       "      <td>-0.436525</td>\n",
       "      <td>-2.553468</td>\n",
       "      <td>0.814685</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.672900</td>\n",
       "      <td>0.446658</td>\n",
       "      <td>1.400807</td>\n",
       "      <td>1.251898</td>\n",
       "      <td>-0.469944</td>\n",
       "      <td>-0.631198</td>\n",
       "      <td>-1.312273</td>\n",
       "      <td>-0.405767</td>\n",
       "      <td>-0.699345</td>\n",
       "      <td>-4.344109e-01</td>\n",
       "      <td>-0.959991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5625</th>\n",
       "      <td>1.729282</td>\n",
       "      <td>-0.449975</td>\n",
       "      <td>-0.022733</td>\n",
       "      <td>-1.308078</td>\n",
       "      <td>-0.715286</td>\n",
       "      <td>1.722175</td>\n",
       "      <td>-0.394474</td>\n",
       "      <td>0.814685</td>\n",
       "      <td>0.097069</td>\n",
       "      <td>-1.649551</td>\n",
       "      <td>-0.261904</td>\n",
       "      <td>-1.497592</td>\n",
       "      <td>-0.253481</td>\n",
       "      <td>0.691336</td>\n",
       "      <td>-0.631198</td>\n",
       "      <td>0.638888</td>\n",
       "      <td>-0.405767</td>\n",
       "      <td>-0.351066</td>\n",
       "      <td>-1.529638e-01</td>\n",
       "      <td>-0.533071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5626</th>\n",
       "      <td>1.729897</td>\n",
       "      <td>-0.449975</td>\n",
       "      <td>0.336401</td>\n",
       "      <td>0.100852</td>\n",
       "      <td>-0.715286</td>\n",
       "      <td>-0.316597</td>\n",
       "      <td>-0.394474</td>\n",
       "      <td>0.814685</td>\n",
       "      <td>0.097069</td>\n",
       "      <td>1.280401</td>\n",
       "      <td>-1.679027</td>\n",
       "      <td>1.400807</td>\n",
       "      <td>-0.253481</td>\n",
       "      <td>0.691336</td>\n",
       "      <td>-0.631198</td>\n",
       "      <td>0.081413</td>\n",
       "      <td>-0.405767</td>\n",
       "      <td>-0.351066</td>\n",
       "      <td>-2.499752e-16</td>\n",
       "      <td>0.971315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5627</th>\n",
       "      <td>1.730513</td>\n",
       "      <td>-0.449975</td>\n",
       "      <td>-1.100136</td>\n",
       "      <td>0.100852</td>\n",
       "      <td>-0.715286</td>\n",
       "      <td>-0.556452</td>\n",
       "      <td>0.325191</td>\n",
       "      <td>0.814685</td>\n",
       "      <td>0.097069</td>\n",
       "      <td>-1.649551</td>\n",
       "      <td>-0.261904</td>\n",
       "      <td>0.676207</td>\n",
       "      <td>-0.253481</td>\n",
       "      <td>-0.469944</td>\n",
       "      <td>1.584290</td>\n",
       "      <td>1.475100</td>\n",
       "      <td>-0.405767</td>\n",
       "      <td>-0.351066</td>\n",
       "      <td>-1.529638e-01</td>\n",
       "      <td>0.178463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5628</th>\n",
       "      <td>1.731128</td>\n",
       "      <td>-0.449975</td>\n",
       "      <td>1.533515</td>\n",
       "      <td>-1.308078</td>\n",
       "      <td>1.469771</td>\n",
       "      <td>-0.796308</td>\n",
       "      <td>-0.394474</td>\n",
       "      <td>0.814685</td>\n",
       "      <td>1.514862</td>\n",
       "      <td>1.280401</td>\n",
       "      <td>-0.261904</td>\n",
       "      <td>0.676207</td>\n",
       "      <td>-0.253481</td>\n",
       "      <td>-0.082850</td>\n",
       "      <td>-0.631198</td>\n",
       "      <td>-0.197324</td>\n",
       "      <td>0.134518</td>\n",
       "      <td>-0.351066</td>\n",
       "      <td>1.254272e+00</td>\n",
       "      <td>0.036156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5629</th>\n",
       "      <td>1.731743</td>\n",
       "      <td>-0.449975</td>\n",
       "      <td>-0.262156</td>\n",
       "      <td>0.100852</td>\n",
       "      <td>-0.715286</td>\n",
       "      <td>-0.076741</td>\n",
       "      <td>-0.394474</td>\n",
       "      <td>0.814685</td>\n",
       "      <td>0.097069</td>\n",
       "      <td>-1.649551</td>\n",
       "      <td>-0.261904</td>\n",
       "      <td>-0.048392</td>\n",
       "      <td>-0.253481</td>\n",
       "      <td>-0.082850</td>\n",
       "      <td>-0.631198</td>\n",
       "      <td>-0.754799</td>\n",
       "      <td>0.134518</td>\n",
       "      <td>-0.351066</td>\n",
       "      <td>-4.344109e-01</td>\n",
       "      <td>-0.167139</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5157 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CustomerID     Churn    Tenure  PreferredLoginDevice  CityTier  \\\n",
       "0      -1.731743  2.222345 -0.741002              0.100852  1.469771   \n",
       "1      -1.731128  2.222345  0.000000              1.509782 -0.715286   \n",
       "2      -1.730513  2.222345  0.000000              1.509782 -0.715286   \n",
       "3      -1.729897  2.222345 -1.219847              1.509782  1.469771   \n",
       "4      -1.729282  2.222345 -1.219847              1.509782 -0.715286   \n",
       "...          ...       ...       ...                   ...       ...   \n",
       "5625    1.729282 -0.449975 -0.022733             -1.308078 -0.715286   \n",
       "5626    1.729897 -0.449975  0.336401              0.100852 -0.715286   \n",
       "5627    1.730513 -0.449975 -1.100136              0.100852 -0.715286   \n",
       "5628    1.731128 -0.449975  1.533515             -1.308078  1.469771   \n",
       "5629    1.731743 -0.449975 -0.262156              0.100852 -0.715286   \n",
       "\n",
       "      WarehouseToHome  PreferredPaymentMode    Gender  HourSpendOnApp  \\\n",
       "0           -1.156091              0.325191 -1.227468        0.097069   \n",
       "1           -0.916235              1.764521  0.814685        0.097069   \n",
       "2            1.722175              0.325191  0.814685       -1.320723   \n",
       "3           -0.076741              0.325191  0.814685       -1.320723   \n",
       "4           -0.436525             -2.553468  0.814685        0.000000   \n",
       "...               ...                   ...       ...             ...   \n",
       "5625         1.722175             -0.394474  0.814685        0.097069   \n",
       "5626        -0.316597             -0.394474  0.814685        0.097069   \n",
       "5627        -0.556452              0.325191  0.814685        0.097069   \n",
       "5628        -0.796308             -0.394474  0.814685        1.514862   \n",
       "5629        -0.076741             -0.394474  0.814685        0.097069   \n",
       "\n",
       "      NumberOfDeviceRegistered  PreferedOrderCat  SatisfactionScore  \\\n",
       "0                    -0.672900         -0.261904          -0.772992   \n",
       "1                     0.303750          0.446658          -0.048392   \n",
       "2                     0.303750          0.446658          -0.048392   \n",
       "3                     0.303750         -0.261904           1.400807   \n",
       "4                    -0.672900          0.446658           1.400807   \n",
       "...                        ...               ...                ...   \n",
       "5625                 -1.649551         -0.261904          -1.497592   \n",
       "5626                  1.280401         -1.679027           1.400807   \n",
       "5627                 -1.649551         -0.261904           0.676207   \n",
       "5628                  1.280401         -0.261904           0.676207   \n",
       "5629                 -1.649551         -0.261904          -0.048392   \n",
       "\n",
       "      MaritalStatus  NumberOfAddress  Complain  OrderAmountHikeFromlastYear  \\\n",
       "0          1.251898         1.852616  1.584290                    -1.312273   \n",
       "1          1.251898         1.078430  1.584290                    -0.197324   \n",
       "2          1.251898         0.691336  1.584290                    -0.476062   \n",
       "3          1.251898         1.465523 -0.631198                     2.032574   \n",
       "4          1.251898        -0.469944 -0.631198                    -1.312273   \n",
       "...             ...              ...       ...                          ...   \n",
       "5625      -0.253481         0.691336 -0.631198                     0.638888   \n",
       "5626      -0.253481         0.691336 -0.631198                     0.081413   \n",
       "5627      -0.253481        -0.469944  1.584290                     1.475100   \n",
       "5628      -0.253481        -0.082850 -0.631198                    -0.197324   \n",
       "5629      -0.253481        -0.082850 -0.631198                    -0.754799   \n",
       "\n",
       "      CouponUsed  OrderCount  DaySinceLastOrder  CashbackAmount  \n",
       "0      -0.405767   -0.699345       1.284833e-01       -0.350105  \n",
       "1      -0.946053   -0.699345      -1.278752e+00       -1.142957  \n",
       "2      -0.946053   -0.699345      -4.344109e-01       -1.163287  \n",
       "3      -0.946053   -0.699345      -4.344109e-01       -0.878673  \n",
       "4      -0.405767   -0.699345      -4.344109e-01       -0.959991  \n",
       "...          ...         ...                ...             ...  \n",
       "5625   -0.405767   -0.351066      -1.529638e-01       -0.533071  \n",
       "5626   -0.405767   -0.351066      -2.499752e-16        0.971315  \n",
       "5627   -0.405767   -0.351066      -1.529638e-01        0.178463  \n",
       "5628    0.134518   -0.351066       1.254272e+00        0.036156  \n",
       "5629    0.134518   -0.351066      -4.344109e-01       -0.167139  \n",
       "\n",
       "[5157 rows x 20 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply data balancing technique\n",
    "X_balanced, y_balanced = apply_data_balancing_technique(data_processed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
